import os
import glob
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import monai
from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, RandRotate90d, RandFlipd, ScaleIntensityd

# --- CONFIGURATION EXPERT ---
HOME = os.path.expanduser("~")
DATA_DIR = os.path.join(HOME, "Alzheimer_Project", "data", "final")
CSV_PATH = os.path.join(HOME, "Alzheimer_Project", "data", "metadata.csv")
MODEL_SAVE_PATH = os.path.join(HOME, "Alzheimer_Project", "models", "alzheimer_cnn.pth")
EPOCHS = 20 # Nombre de fois o√π l'IA voit tout le dataset
BATCH_SIZE = 4 # Petit batch car les images 3D prennent beaucoup de VRAM
LR = 0.0001 # Learning Rate doux pour apprendre finement

# --- 1. LE MOD√àLE (Architecture CNN 3D Optimis√©e) ---
class ExpertAlzheimer3D(nn.Module):
    def __init__(self):
        super(ExpertAlzheimer3D, self).__init__()
        # Block 1
        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm3d(16)
        self.pool1 = nn.MaxPool3d(2) # 96 -> 48
        
        # Block 2
        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm3d(32)
        self.pool2 = nn.MaxPool3d(2) # 48 -> 24
        
        # Block 3
        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm3d(64)
        self.pool3 = nn.MaxPool3d(2) # 24 -> 12
        
        # Block 4
        self.conv4 = nn.Conv3d(64, 128, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm3d(128)
        self.pool4 = nn.MaxPool3d(2) # 12 -> 6
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5) # Anti-overfitting crucial
        
        # Classifier
        self.fc1 = nn.Linear(128 * 6 * 6 * 6, 512)
        self.fc2 = nn.Linear(512, 2) # 2 Classes: CN (0) vs AD (1)

    def forward(self, x):
        x = self.pool1(self.relu(self.bn1(self.conv1(x))))
        x = self.pool2(self.relu(self.bn2(self.conv2(x))))
        x = self.pool3(self.relu(self.bn3(self.conv3(x))))
        x = self.pool4(self.relu(self.bn4(self.conv4(x))))
        
        x = x.view(x.size(0), -1) # Flatten
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# --- 2. GESTION DES DONN√âES ---
def get_data_loaders():
    # A. R√©cup√©rer les labels du CSV
    df = pd.read_csv(CSV_PATH)
    label_map = dict(zip(df['ImageID'], df['Group']))
    
    # B. Lister les fichiers disponibles
    files = glob.glob(os.path.join(DATA_DIR, "*.nii.gz"))
    data_list = []
    labels_list = []
    
    print(f"üîç Scan des fichiers dans {DATA_DIR}...")
    for f in files:
        img_id = os.path.basename(f).replace(".nii.gz", "")
        if img_id in label_map:
            group = label_map[img_id]
            if group in ['CN', 'AD']:
                data_list.append({"image": f})
                # AD = 1, CN = 0
                labels_list.append(1 if group == 'AD' else 0)
    
    print(f"üìä Dataset Final: {len(data_list)} images (CN/AD)")
    
    # C. Split Train / Val (80% / 20%)
    X_train, X_val, y_train, y_val = train_test_split(data_list, labels_list, test_size=0.2, random_state=42, stratify=labels_list)
    
    # D. Augmentation de donn√©es (Pour √©viter le par-c≈ìur)
    train_transforms = Compose([
        LoadImaged(keys=["image"]),
        EnsureChannelFirstd(keys=["image"]),
        ScaleIntensityd(keys=["image"]),
        RandRotate90d(keys=["image"], prob=0.5, spatial_axes=(0, 2)), # Rotation al√©atoire
        RandFlipd(keys=["image"], prob=0.5, spatial_axis=0), # Miroir al√©atoire
    ])
    
    val_transforms = Compose([
        LoadImaged(keys=["image"]),
        EnsureChannelFirstd(keys=["image"]),
        ScaleIntensityd(keys=["image"])
    ])

    # Cr√©ation des Datasets MONAI
    train_ds = monai.data.Dataset(data=X_train, transform=train_transforms)
    val_ds = monai.data.Dataset(data=X_val, transform=val_transforms)

    # Cr√©ation des DataLoaders (Chargement parall√®le)
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    
    return train_loader, val_loader, y_train, y_val

# --- 3. ENTRA√éNEMENT ---
def train():
    if not os.path.exists(os.path.dirname(MODEL_SAVE_PATH)):
        os.makedirs(os.path.dirname(MODEL_SAVE_PATH))

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üî• D√©marrage sur : {device}")
    
    try:
        train_loader, val_loader, _, _ = get_data_loaders()
    except ValueError:
        print("‚ùå Erreur : Pas assez d'images trouv√©es. Attends la fin du script 'prepare_dataset.py' !")
        return

    model = ExpertAlzheimer3D().to(device)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    criterion = nn.CrossEntropyLoss() # Poids pour d√©s√©quilibre de classes possible ici

    print("üöÄ Start Training...")
    
    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch in train_loader:
            inputs = batch["image"].to(device)
            # Les labels doivent √™tre un tenseur simple
            # Astuce: on r√©cup√®re les labels depuis le filename ou le loader, 
            # mais ici on doit ruser car monai Dataset dict ne porte pas le label par d√©faut.
            # SIMPLIFICATION EXPERTE : On va recr√©er le label √† la vol√©e pour √©viter la complexit√© MONAI pure.
            # Note: Dans ce script simplifi√©, il manque l'association directe Image <-> Label dans le loader MONAI.
            # Je vais corriger √ßa dans la version suivante si √ßa plante, mais testons l'architecture d'abord.
            pass 

    # --- STOP --- 
    # Je r√©alise que le code ci-dessus pour les labels avec MONAI Dataset est un peu pi√©geux
    # car MONAI g√®re les images, mais il faut lui passer les labels explicitement.
    # Je vais te donner la version CORRIG√âE ci-dessous pour ne pas que tu perdes de temps.

if __name__ == "__main__":
    pass